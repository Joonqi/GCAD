{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joonqi/GCAD/blob/main/train_GCAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYfGesyD4KSZ"
      },
      "source": [
        "## Global Context Anomaly Detection\n",
        "Method to solve MVTec LOCO dataset -> by extracting logically constrained features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nbTk_qz4KSc"
      },
      "outputs": [],
      "source": [
        "# '''\n",
        "# https://github.com/denguir/student-teacher-anomaly-detection\n",
        "# https://github.com/erezposner/Fast_Dense_Feature_Extraction\n",
        "# https://discuss.pytorch.org/t/unet-implementation/426\n",
        "# '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7fTmB09PBgk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !pip install einops\n",
        "# !pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TzHC5JZOJj6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchsummary\n",
        "# import pytorch_lightning as pl\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "from PIL import Image\n",
        "torch.manual_seed(42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHr08Ahk_VO4",
        "outputId": "23e8451a-aaa4-4a3d-d800-224338382a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXnta1g-4KSf",
        "outputId": "3f43eeba-abe4-4b30-81fc-ea1983162cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69601607\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "time_stamp = str(int(time.time()))[2:]\n",
        "print(time_stamp)\n",
        "\n",
        "target_dataset = 'juice_bottle'\n",
        "\n",
        "d_glo = 10\n",
        "d_loc = 128\n",
        "\n",
        "epochs = 125\n",
        "early_stopping = 30\n",
        "\n",
        "os.makedirs(f'/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}', exist_ok=True)\n",
        "os.makedirs(f'/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/results', exist_ok=True)\n",
        "for mode in ['both', 'global', 'local']:\n",
        "    os.makedirs(f'/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/{mode}/results', exist_ok=True)\n",
        "    \n",
        "mode = 'both'\n",
        "assert (mode in ['both', 'local', 'global'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tNV2JacQZxy",
        "outputId": "a104cf36-04a8-4990-cee2-fc995a164f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8bEOaeh4KSf"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]= '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRiyajP84KSg"
      },
      "outputs": [],
      "source": [
        "# Source:\n",
        "# https://github.com/erezposner/Fast_Dense_Feature_Extraction\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# (N,C,H,W)\n",
        "\n",
        "\n",
        "class multiPoolPrepare(nn.Module):\n",
        "    def __init__(self, patchY, patchX):\n",
        "        super(multiPoolPrepare, self).__init__()\n",
        "        pady = patchY - 1\n",
        "        padx = patchX - 1\n",
        "\n",
        "        self.pad_top = np.ceil(pady / 2).astype(int)\n",
        "        self.pad_bottom = np.floor(pady / 2).astype(int)\n",
        "        self.pad_left = np.ceil(padx / 2).astype(int)\n",
        "        self.pad_right = np.floor(padx / 2).astype(int)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.pad(x, [self.pad_left, self.pad_right, self.pad_top, self.pad_bottom], value=0)\n",
        "        return y\n",
        "\n",
        "\n",
        "class unwrapPrepare(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(unwrapPrepare, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_ = F.pad(x, [0, -1, 0, -1], value=0)\n",
        "        y = x_.contiguous().view(x_.shape[0], -1)\n",
        "        y = y.transpose(0, 1)\n",
        "        return y.contiguous()\n",
        "\n",
        "\n",
        "class unwrapPool(nn.Module):\n",
        "    def __init__(self, outChans, curImgW, curImgH, dW, dH):\n",
        "        super(unwrapPool, self).__init__()\n",
        "        self.outChans = int(outChans)\n",
        "        self.curImgW = int(curImgW)\n",
        "        self.curImgH = int(curImgH)\n",
        "        self.dW = int(dW)\n",
        "        self.dH = int(dH)\n",
        "\n",
        "    def forward(self, x, ):\n",
        "        y = x.view((self.outChans, self.curImgW, self.curImgH, self.dH, self.dW, -1))\n",
        "        y = y.transpose(2, 3)\n",
        "\n",
        "        return y.contiguous()\n",
        "\n",
        "\n",
        "class multiMaxPooling(nn.Module):\n",
        "    def __init__(self, kW, kH, dW, dH):\n",
        "        super(multiMaxPooling, self).__init__()\n",
        "        layers = []\n",
        "        self.padd = []\n",
        "        for i in range(0, dH):\n",
        "            for j in range(0, dW):\n",
        "                self.padd.append((-j, -i))\n",
        "                layers.append(nn.MaxPool2d(kernel_size=(kW, kH), stride=(dW, dH)))\n",
        "        self.max_layers = nn.ModuleList(layers)\n",
        "        self.s = dH\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        hh = []\n",
        "        ww = []\n",
        "        res = []\n",
        "\n",
        "        for i in range(0, len(self.max_layers)):\n",
        "            pad_left, pad_top = self.padd[i]\n",
        "            _x = F.pad(x, [pad_left, pad_left, pad_top, pad_top], value=0)\n",
        "            _x = self.max_layers[i](_x)\n",
        "            h, w = _x.size()[2], _x.size()[3]\n",
        "            hh.append(h)\n",
        "            ww.append(w)\n",
        "            res.append(_x)\n",
        "        max_h, max_w = np.max(hh), np.max(ww)\n",
        "        for i in range(0, len(self.max_layers)):\n",
        "            _x = res[i]\n",
        "            h, w = _x.size()[2], _x.size()[3]\n",
        "            pad_top = np.floor((max_h - h) / 2).astype(int)\n",
        "            pad_bottom = np.ceil((max_h - h) / 2).astype(int)\n",
        "            pad_left = np.floor((max_w - w) / 2).astype(int)\n",
        "            pad_right = np.ceil((max_w - w) / 2).astype(int)\n",
        "            _x = F.pad(_x, [pad_left, pad_right, pad_top, pad_bottom], value=0)\n",
        "            res[i] = _x\n",
        "        return torch.cat(res, 0)\n",
        "\n",
        "\n",
        "class multiConv(nn.Module):\n",
        "    def __init__(self, nInputPlane, nOutputPlane, kW, kH, dW, dH):\n",
        "        super(multiConv, self).__init__()\n",
        "        layers = []\n",
        "        self.padd = []\n",
        "        for i in range(0, dH):\n",
        "            for j in range(0, dW):\n",
        "                self.padd.append((-j, -i))\n",
        "                torch.manual_seed(10)\n",
        "                torch.cuda.manual_seed(10)\n",
        "                a = nn.Conv2d(nInputPlane, nOutputPlane, kernel_size=(kW, kH), stride=(dW, dH), padding=0)\n",
        "                layers.append(a)\n",
        "        self.max_layers = nn.ModuleList(layers)\n",
        "        self.s = dW\n",
        "\n",
        "    def forward(self, x):\n",
        "        hh = []\n",
        "        ww = []\n",
        "        res = []\n",
        "\n",
        "        for i in range(0, len(self.max_layers)):\n",
        "            pad_left, pad_top = self.padd[i]\n",
        "            _x = F.pad(x, [pad_left, pad_left, pad_top, pad_top], value=0)\n",
        "            _x = self.max_layers[i](_x)\n",
        "            h, w = _x.size()[2], _x.size()[3]\n",
        "            hh.append(h)\n",
        "            ww.append(w)\n",
        "            res.append(_x)\n",
        "        max_h, max_w = np.max(hh), np.max(ww)\n",
        "        for i in range(0, len(self.max_layers)):\n",
        "            _x = res[i]\n",
        "            h, w = _x.size()[2], _x.size()[3]\n",
        "            pad_top = np.ceil((max_h - h) / 2).astype(int)\n",
        "            pad_bottom = np.floor((max_h - h) / 2).astype(int)\n",
        "            pad_left = np.ceil((max_w - w) / 2).astype(int)\n",
        "            pad_right = np.floor((max_w - w) / 2).astype(int)\n",
        "            _x = F.pad(_x, [pad_left, pad_right, pad_top, pad_bottom], value=0)\n",
        "            res[i] = _x\n",
        "        return torch.cat(res, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBxOMJbXG87w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b1a436-1601-4dfb-bdea-28f2aba047eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalBranchEncoder on Patches (33x33)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 128, 29, 29]           9,728\n",
            "         LeakyReLU-2          [-1, 128, 29, 29]               0\n",
            "         MaxPool2d-3          [-1, 128, 14, 14]               0\n",
            "            Conv2d-4          [-1, 256, 10, 10]         819,456\n",
            "         LeakyReLU-5          [-1, 256, 10, 10]               0\n",
            "         MaxPool2d-6            [-1, 256, 5, 5]               0\n",
            "            Conv2d-7            [-1, 256, 4, 4]         262,400\n",
            "         LeakyReLU-8            [-1, 256, 4, 4]               0\n",
            "            Conv2d-9            [-1, 128, 1, 1]         524,416\n",
            "        LeakyReLU-10            [-1, 128, 1, 1]               0\n",
            "        Dropout2d-11            [-1, 128, 1, 1]               0\n",
            "           Linear-12                  [-1, 512]          66,048\n",
            "        LeakyReLU-13                  [-1, 512]               0\n",
            "          Dropout-14                  [-1, 512]               0\n",
            "================================================================\n",
            "Total params: 1,682,048\n",
            "Trainable params: 1,682,048\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.35\n",
            "Params size (MB): 6.42\n",
            "Estimated Total Size (MB): 8.78\n",
            "----------------------------------------------------------------\n",
            "LocalBranchEncoder on whole image (256x256)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "  multiPoolPrepare-1          [-1, 3, 288, 288]               0\n",
            "            Conv2d-2        [-1, 128, 284, 284]           9,728\n",
            "         LeakyReLU-3        [-1, 128, 284, 284]               0\n",
            "         MaxPool2d-4        [-1, 128, 142, 142]               0\n",
            "         MaxPool2d-5        [-1, 128, 142, 141]               0\n",
            "         MaxPool2d-6        [-1, 128, 141, 142]               0\n",
            "         MaxPool2d-7        [-1, 128, 141, 141]               0\n",
            "   multiMaxPooling-8        [-1, 128, 142, 142]               0\n",
            "            Conv2d-9        [-1, 256, 138, 138]         819,456\n",
            "        LeakyReLU-10        [-1, 256, 138, 138]               0\n",
            "        MaxPool2d-11          [-1, 256, 69, 69]               0\n",
            "        MaxPool2d-12          [-1, 256, 69, 68]               0\n",
            "        MaxPool2d-13          [-1, 256, 68, 69]               0\n",
            "        MaxPool2d-14          [-1, 256, 68, 68]               0\n",
            "  multiMaxPooling-15          [-1, 256, 69, 69]               0\n",
            "           Conv2d-16          [-1, 256, 68, 68]         262,400\n",
            "        LeakyReLU-17          [-1, 256, 68, 68]               0\n",
            "           Conv2d-18          [-1, 128, 65, 65]         524,416\n",
            "        LeakyReLU-19          [-1, 128, 65, 65]               0\n",
            "    unwrapPrepare-20                   [-1, 32]               0\n",
            "           Linear-21        [-1, 256, 256, 512]          66,048\n",
            "        LeakyReLU-22        [-1, 256, 256, 512]               0\n",
            "================================================================\n",
            "Total params: 1,682,048\n",
            "Trainable params: 1,682,048\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 916.00\n",
            "Params size (MB): 6.42\n",
            "Estimated Total Size (MB): 923.16\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class LocalBranchEncoder(nn.Module):\n",
        "    def __init__(self, fdfe=False):\n",
        "        super(LocalBranchEncoder, self).__init__()\n",
        "        self.pH = 33\n",
        "        self.pW = 33\n",
        "        self.bool_fdfe = fdfe\n",
        "        self.multiPoolPrepare = multiPoolPrepare(self.pH, self.pW)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 128, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(128, 256, 5, 1)\n",
        "        self.conv3 = nn.Conv2d(256, 256, 2, 1)\n",
        "        self.conv4 = nn.Conv2d(256, 128, 4, 1)\n",
        "        self.outChans = self.conv4.out_channels\n",
        "        self.decode = nn.Linear(128, 512)\n",
        "\n",
        "        self.dropout_2d = nn.Dropout2d(0.2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        self.multiMaxPooling = multiMaxPooling(2, 2, 2, 2)\n",
        "        self.unwrapPrepare = unwrapPrepare()\n",
        "\n",
        "        self.l_relu = nn.LeakyReLU(5e-3)\n",
        "\n",
        "    def fdfe(self, x):\n",
        "        '''Use Fast Dense Feature Extraction to efficiently apply \n",
        "        the patch-based CNN AnomalyNet33 on a whole image.'''\n",
        "\n",
        "        imH = x.size(2)\n",
        "        imW = x.size(3)\n",
        "\n",
        "        unwrapPool2 = unwrapPool(self.outChans, imH / (2 * 2), imW / (2 * 2), 2, 2)\n",
        "        unwrapPool1 = unwrapPool(self.outChans, imH / 2, imW / 2, 2, 2)\n",
        "\n",
        "        x = self.multiPoolPrepare(x)\n",
        "\n",
        "        x = self.l_relu(self.conv1(x))\n",
        "        x = self.multiMaxPooling(x)\n",
        "\n",
        "        x = self.l_relu(self.conv2(x))\n",
        "        x = self.multiMaxPooling(x)\n",
        "\n",
        "        x = self.l_relu(self.conv3(x))\n",
        "        x = self.l_relu(self.conv4(x))\n",
        "\n",
        "        x = self.unwrapPrepare(x)\n",
        "        x = unwrapPool2(x)\n",
        "        x = unwrapPool1(x)\n",
        "\n",
        "        y = x.view(self.outChans, imH, imW, -1)\n",
        "        y = y.permute(3, 1, 2, 0)\n",
        "        y = self.l_relu(self.decode(y))\n",
        "        return y\n",
        "\n",
        "    def forward(self, x, fdfe=False):\n",
        "        if (fdfe) or (self.bool_fdfe):\n",
        "            return self.fdfe(x)\n",
        "        else:\n",
        "            assert x.size(2) == self.pH and x.size(3) == self.pW, \\\n",
        "                f\"This patch extractor only accepts input of size (b, 3, {self.pH}, {self.pW})\"\n",
        "            x = self.l_relu(self.conv1(x))\n",
        "            x = self.max_pool(x)\n",
        "            x = self.l_relu(self.conv2(x))\n",
        "            x = self.max_pool(x)\n",
        "            x = self.l_relu(self.conv3(x))\n",
        "            x = self.l_relu(self.conv4(x))\n",
        "            x = self.dropout_2d(x)\n",
        "            x = x.view(-1, self.outChans)\n",
        "            x = self.l_relu(self.decode(x))\n",
        "            x = self.dropout(x)\n",
        "            return x\n",
        "        \n",
        "print(\"LocalBranchEncoder on Patches (33x33)\")\n",
        "model = LocalBranchEncoder().cuda()\n",
        "torchsummary.summary(model, (3, 33, 33))\n",
        "del model\n",
        "\n",
        "print(\"LocalBranchEncoder on whole image (256x256)\")\n",
        "model = LocalBranchEncoder(fdfe=True).cuda()\n",
        "torchsummary.summary(model, (3, 256, 256))\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jwOhZhlOMv9",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66533c6e-f232-4c17-f246-3ff7a5d987b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 128, 128]           1,568\n",
            "         LeakyReLU-2         [-1, 32, 128, 128]               0\n",
            "            Conv2d-3         [-1, 32, 128, 128]           1,056\n",
            "         LeakyReLU-4         [-1, 32, 128, 128]               0\n",
            "            Conv2d-5           [-1, 32, 64, 64]          16,416\n",
            "         LeakyReLU-6           [-1, 32, 64, 64]               0\n",
            "            Conv2d-7           [-1, 32, 64, 64]           1,056\n",
            "         LeakyReLU-8           [-1, 32, 64, 64]               0\n",
            "            Conv2d-9           [-1, 64, 32, 32]          32,832\n",
            "        LeakyReLU-10           [-1, 64, 32, 32]               0\n",
            "           Conv2d-11           [-1, 32, 32, 32]           2,080\n",
            "        LeakyReLU-12           [-1, 32, 32, 32]               0\n",
            "           Conv2d-13           [-1, 64, 16, 16]          65,600\n",
            "        LeakyReLU-14           [-1, 64, 16, 16]               0\n",
            "           Conv2d-15           [-1, 32, 16, 16]           2,080\n",
            "        LeakyReLU-16           [-1, 32, 16, 16]               0\n",
            "           Conv2d-17             [-1, 64, 8, 8]          65,600\n",
            "        LeakyReLU-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19             [-1, 32, 8, 8]           2,080\n",
            "        LeakyReLU-20             [-1, 32, 8, 8]               0\n",
            "           Conv2d-21             [-1, 32, 2, 2]         131,104\n",
            "        LeakyReLU-22             [-1, 32, 2, 2]               0\n",
            "  ConvTranspose2d-23             [-1, 32, 8, 8]          65,568\n",
            "        LeakyReLU-24             [-1, 64, 8, 8]               0\n",
            "  ConvTranspose2d-25           [-1, 32, 16, 16]          32,800\n",
            "        LeakyReLU-26           [-1, 64, 16, 16]               0\n",
            "  ConvTranspose2d-27           [-1, 32, 32, 32]          32,800\n",
            "        LeakyReLU-28           [-1, 64, 32, 32]               0\n",
            "  ConvTranspose2d-29           [-1, 32, 64, 64]          32,800\n",
            "        LeakyReLU-30           [-1, 64, 64, 64]               0\n",
            "  ConvTranspose2d-31         [-1, 32, 128, 128]          32,800\n",
            "        LeakyReLU-32         [-1, 64, 128, 128]               0\n",
            "  ConvTranspose2d-33         [-1, 32, 256, 256]          32,800\n",
            "        LeakyReLU-34         [-1, 32, 256, 256]               0\n",
            "           Conv2d-35        [-1, 128, 256, 256]           4,224\n",
            "        LeakyReLU-36        [-1, 128, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 555,264\n",
            "Trainable params: 555,264\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 197.96\n",
            "Params size (MB): 2.12\n",
            "Estimated Total Size (MB): 200.82\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# class GlobalBranchEncoder(pl.LightningModule):\n",
        "\n",
        "class GlobalBranchEncoder(nn.Module):\n",
        "    def __init__(self, decode=True, upsample=False):\n",
        "        super(GlobalBranchEncoder, self).__init__()\n",
        "        self.g_dim = 32\n",
        "        self.decode = decode\n",
        "        self.bool_upsample = upsample\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, (4, 4), 2, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, (4, 4), 2, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, (4, 4), 2, 1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, (4, 4), 2, 1)\n",
        "        self.conv5 = nn.Conv2d(64, 64, (4, 4), 2, 1)\n",
        "        self.conv6 = nn.Conv2d(64, self.g_dim, (8, 8), 2, 1)\n",
        "            \n",
        "        self.upconv1 = nn.ConvTranspose2d(self.g_dim, 32, (8, 8), 2, 1)\n",
        "        self.upconv2 = nn.ConvTranspose2d(64, 32, (4, 4), 2, 1)\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 32, (4, 4), 2, 1)\n",
        "        self.upconv4 = nn.ConvTranspose2d(64, 32, (4, 4), 2, 1)\n",
        "        self.upconv5 = nn.ConvTranspose2d(64, 32, (4, 4), 2, 1)\n",
        "        self.upconv6 = nn.ConvTranspose2d(64, 32, (4, 4), 2, 1)\n",
        "        \n",
        "        self.skip_conn1 = nn.Conv2d(32, 32, (1, 1), 1)\n",
        "        self.skip_conn2 = nn.Conv2d(32, 32, (1, 1), 1)\n",
        "        self.skip_conn3 = nn.Conv2d(64, 32, (1, 1), 1)\n",
        "        self.skip_conn4 = nn.Conv2d(64, 32, (1, 1), 1)\n",
        "        self.skip_conn5 = nn.Conv2d(64, 32, (1, 1), 1)\n",
        "\n",
        "        self.decoder = nn.Conv2d(32, d_glo, (1, 1), 1)\n",
        "        self.upsample = nn.Conv2d(32, d_loc, (1, 1), 1)\n",
        "        self.l_relu = nn.LeakyReLU()\n",
        "        \n",
        "        self.dropout_2d = nn.Dropout2d(0.2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        self.multiMaxPooling = multiMaxPooling(2, 2, 2, 2)\n",
        "        self.unwrapPrepare = unwrapPrepare()\n",
        "        self.outChans = 128\n",
        "    \n",
        "    def forward(self, input, fdfe=True, upsample=False):            \n",
        "        x = self.conv1(input)\n",
        "        x = self.l_relu(x)\n",
        "        _u = self.l_relu(self.skip_conn1(x))\n",
        "        x = self.conv2(x)\n",
        "        x = self.l_relu(x)\n",
        "        _v = self.l_relu(self.skip_conn2(x))\n",
        "        x = self.conv3(x)\n",
        "        x = self.l_relu(x)\n",
        "        _w = self.l_relu(self.skip_conn3(x))\n",
        "        x = self.conv4(x)\n",
        "        x = self.l_relu(x)\n",
        "        _x = self.l_relu(self.skip_conn4(x))\n",
        "        x = self.conv5(x)\n",
        "        x = self.l_relu(x)\n",
        "        _y = self.l_relu(self.skip_conn5(x))\n",
        "        x = self.conv6(x)\n",
        "        g = self.l_relu(x)\n",
        "\n",
        "        x = torch.cat((self.upconv1(g), _y), 1)\n",
        "        x = self.l_relu(x)    \n",
        "        x = torch.cat((self.upconv2(x), _x), 1)\n",
        "        x = self.l_relu(x)\n",
        "        x = torch.cat((self.upconv3(x), _w), 1)\n",
        "        x = self.l_relu(x)\n",
        "        x = torch.cat((self.upconv4(x), _v), 1)\n",
        "        x = self.l_relu(x)\n",
        "        x = torch.cat((self.upconv5(x), _u), 1)\n",
        "        x = self.l_relu(x)\n",
        "        x = self.upconv6(x)\n",
        "        x = self.l_relu(x)\n",
        "\n",
        "        if (upsample) or (self.bool_upsample):\n",
        "            output = self.l_relu(self.upsample(x))\n",
        "        elif self.decode:\n",
        "            output = self.l_relu(self.decoder(x))\n",
        "        else:\n",
        "            output = x        \n",
        "      \n",
        "        return output\n",
        "\n",
        "model = GlobalBranchEncoder(upsample=True)\n",
        "# model = GlobalBranchEncoder()\n",
        "\n",
        "torchsummary.summary(model.cuda(), (3, 256, 256))\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2loI200bWu2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7dfe3c-c4d3-4251-a73c-9ea60b0cf195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
            "              ReLU-2         [-1, 64, 256, 256]               0\n",
            "            Conv2d-3         [-1, 64, 256, 256]          36,928\n",
            "              ReLU-4         [-1, 64, 256, 256]               0\n",
            "     UNetConvBlock-5         [-1, 64, 256, 256]               0\n",
            "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
            "              ReLU-7        [-1, 128, 128, 128]               0\n",
            "            Conv2d-8        [-1, 128, 128, 128]         147,584\n",
            "              ReLU-9        [-1, 128, 128, 128]               0\n",
            "    UNetConvBlock-10        [-1, 128, 128, 128]               0\n",
            "           Conv2d-11          [-1, 256, 64, 64]         295,168\n",
            "             ReLU-12          [-1, 256, 64, 64]               0\n",
            "           Conv2d-13          [-1, 256, 64, 64]         590,080\n",
            "             ReLU-14          [-1, 256, 64, 64]               0\n",
            "    UNetConvBlock-15          [-1, 256, 64, 64]               0\n",
            "           Conv2d-16          [-1, 512, 32, 32]       1,180,160\n",
            "             ReLU-17          [-1, 512, 32, 32]               0\n",
            "           Conv2d-18          [-1, 512, 32, 32]       2,359,808\n",
            "             ReLU-19          [-1, 512, 32, 32]               0\n",
            "    UNetConvBlock-20          [-1, 512, 32, 32]               0\n",
            "           Conv2d-21         [-1, 1024, 16, 16]       4,719,616\n",
            "             ReLU-22         [-1, 1024, 16, 16]               0\n",
            "           Conv2d-23         [-1, 1024, 16, 16]       9,438,208\n",
            "             ReLU-24         [-1, 1024, 16, 16]               0\n",
            "    UNetConvBlock-25         [-1, 1024, 16, 16]               0\n",
            "         Upsample-26         [-1, 1024, 32, 32]               0\n",
            "           Conv2d-27          [-1, 512, 32, 32]         524,800\n",
            "           Conv2d-28          [-1, 512, 32, 32]       4,719,104\n",
            "             ReLU-29          [-1, 512, 32, 32]               0\n",
            "           Conv2d-30          [-1, 512, 32, 32]       2,359,808\n",
            "             ReLU-31          [-1, 512, 32, 32]               0\n",
            "    UNetConvBlock-32          [-1, 512, 32, 32]               0\n",
            "      UNetUpBlock-33          [-1, 512, 32, 32]               0\n",
            "         Upsample-34          [-1, 512, 64, 64]               0\n",
            "           Conv2d-35          [-1, 256, 64, 64]         131,328\n",
            "           Conv2d-36          [-1, 256, 64, 64]       1,179,904\n",
            "             ReLU-37          [-1, 256, 64, 64]               0\n",
            "           Conv2d-38          [-1, 256, 64, 64]         590,080\n",
            "             ReLU-39          [-1, 256, 64, 64]               0\n",
            "    UNetConvBlock-40          [-1, 256, 64, 64]               0\n",
            "      UNetUpBlock-41          [-1, 256, 64, 64]               0\n",
            "         Upsample-42        [-1, 256, 128, 128]               0\n",
            "           Conv2d-43        [-1, 128, 128, 128]          32,896\n",
            "           Conv2d-44        [-1, 128, 128, 128]         295,040\n",
            "             ReLU-45        [-1, 128, 128, 128]               0\n",
            "           Conv2d-46        [-1, 128, 128, 128]         147,584\n",
            "             ReLU-47        [-1, 128, 128, 128]               0\n",
            "    UNetConvBlock-48        [-1, 128, 128, 128]               0\n",
            "      UNetUpBlock-49        [-1, 128, 128, 128]               0\n",
            "         Upsample-50        [-1, 128, 256, 256]               0\n",
            "           Conv2d-51         [-1, 64, 256, 256]           8,256\n",
            "           Conv2d-52         [-1, 64, 256, 256]          73,792\n",
            "             ReLU-53         [-1, 64, 256, 256]               0\n",
            "           Conv2d-54         [-1, 64, 256, 256]          36,928\n",
            "             ReLU-55         [-1, 64, 256, 256]               0\n",
            "    UNetConvBlock-56         [-1, 64, 256, 256]               0\n",
            "      UNetUpBlock-57         [-1, 64, 256, 256]               0\n",
            "           Conv2d-58        [-1, 128, 256, 256]           8,320\n",
            "================================================================\n",
            "Total params: 28,951,040\n",
            "Trainable params: 28,951,040\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 914.00\n",
            "Params size (MB): 110.44\n",
            "Estimated Total Size (MB): 1025.19\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        n_classes=2,\n",
        "        depth=5,\n",
        "        wf=6,\n",
        "        padding=False,\n",
        "        batch_norm=False,\n",
        "        up_mode='upconv',\n",
        "    ):\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(\n",
        "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(\n",
        "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path) - 1:\n",
        "                blocks.append(x)\n",
        "                x = F.max_pool2d(x, 2)\n",
        "\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "\n",
        "        return self.last(x)\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
        "            )\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[\n",
        "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out\n",
        "        \n",
        "model = UNet(in_channels=3, n_classes=d_loc, padding=True, up_mode='upsample').cuda()\n",
        "torchsummary.summary(model, (3, 256, 256))\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWlwr2_v4KSj"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "class AnomalyDataset(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, root_dir, transform=transforms.ToTensor(), gt_transform=transforms.ToTensor(), **constraint):\n",
        "        super(AnomalyDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.gt_transform = gt_transform\n",
        "        self.img_dir = os.path.join(self.root_dir, 'img')\n",
        "        self.gt_dir = os.path.join(self.root_dir, 'ground_truth')\n",
        "        self.dataset = self.root_dir.split('/')[-1]\n",
        "        self.csv_file =  os.path.join(self.root_dir, self.dataset + '.csv')\n",
        "        self.frame_list = self._get_dataset(self.csv_file, constraint)\n",
        "    \n",
        "    def _get_dataset(self, csv_file, constraint):\n",
        "        '''Apply filter based on the contraint dict on the dataset'''\n",
        "        df = pd.read_csv(csv_file, keep_default_na=False)\n",
        "        df = df.loc[(df[list(constraint)] == pd.Series(constraint)).all(axis=1)]\n",
        "        return df\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.frame_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        item = self.frame_list.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, item['image_name'])\n",
        "        label = self.frame_list.iloc[idx]['label']\n",
        "        image = Image.open(img_path)\n",
        " \n",
        "        if item['gt_name']:\n",
        "            if target_dataset in ['juice_bottle', 'pushpins', 'screw_bag', 'splicing_connectors', 'breakfast_box']:\n",
        "                gt_path = os.path.join(self.gt_dir, item['gt_name'][:-9], '000.png')\n",
        "            else:\n",
        "                gt_path = os.path.join(self.gt_dir, item['gt_name'])\n",
        "            gt = Image.open(gt_path)\n",
        "        \n",
        "        else:\n",
        "            gt = Image.new('L', image.size, color=0)\n",
        "\n",
        "        sample = {'label': label}\n",
        "\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform(image)\n",
        "\n",
        "        if self.gt_transform:\n",
        "            sample['gt'] = self.gt_transform(gt)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtp2j2umsntf"
      },
      "outputs": [],
      "source": [
        "def distillation_loss(output, target):\n",
        "    err = torch.norm(output - target, dim=1)**2\n",
        "    loss = torch.mean(err)\n",
        "    return loss\n",
        "\n",
        "def compactness_loss(output):\n",
        "    _, n = output.size()\n",
        "    avg = torch.mean(output, axis=1)\n",
        "    std = torch.std(output, axis=1)\n",
        "    zt = output.T - avg\n",
        "    zt /= std\n",
        "    corr = torch.matmul(zt.T, zt) / (n-1)\n",
        "    loss = torch.sum(torch.triu(corr, diagonal=1)**2)\n",
        "    return loss\n",
        "\n",
        "def increment_mean_and_var(mu_N, var_N, N, batch):\n",
        "    '''Increment value of mean and variance based on\n",
        "       current mean, var and new batch\n",
        "    '''\n",
        "    # batch: (batch, h, w, vector)\n",
        "    B = batch.size()[0] # batch size\n",
        "    # we want a descriptor vector -> mean over batch and pixels\n",
        "    mu_B = torch.mean(batch, dim=[0,1,2])\n",
        "    S_B = B * torch.var(batch, dim=[0,1,2], unbiased=False) \n",
        "    S_N = N * var_N\n",
        "    mu_NB = N/(N + B) * mu_N + B/(N + B) * mu_B\n",
        "    S_NB = S_N + S_B + B * mu_B**2 + N * mu_N**2 - (N + B) * mu_NB**2\n",
        "    var_NB = S_NB / (N+B)\n",
        "    return mu_NB, var_NB, N + B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sB5rBZ44KSj"
      },
      "outputs": [],
      "source": [
        "def student_loss(output, target):\n",
        "    # dim: (batch, h, w, vector)\n",
        "    err = reduce((output - target)**2, 'b h w vec -> b h w', 'sum')\n",
        "    loss = torch.mean(err)\n",
        "    return loss\n",
        "\n",
        "class Conv1x1(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Conv1x1, self).__init__()\n",
        "        self.conv = nn.Conv2d(input_size, d_loc, (1, 1), 1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBYk5YNe37vb",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fda994-47a2-4aea-afaf-b767c28d1cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing of training dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "42it [03:54,  5.59s/it]\n"
          ]
        }
      ],
      "source": [
        "# Training Global Encoder with MVTec LOCO (target) dataset\n",
        "import torchvision\n",
        "from einops import reduce, rearrange\n",
        "import pandas as pd\n",
        "\n",
        "localEncoder = LocalBranchEncoder()\n",
        "# trained local encoder\n",
        "localEncoder.load_state_dict(torch.load(f\"/content/drive/MyDrive/LOCO_AD/local_encoder_fixed.pt\"))\n",
        "localEncoder.eval().to(torch.device(\"cuda:0\"))\n",
        "localEncoder = nn.DataParallel(localEncoder, output_device=1)\n",
        "\n",
        "globalEncoder = GlobalBranchEncoder()\n",
        "localRegressor = LocalBranchEncoder()\n",
        "globalRegressor = UNet(in_channels=3, n_classes=d_glo, padding=True, up_mode='upsample')\n",
        "conv11 = nn.DataParallel(Conv1x1(512).to(torch.device(\"cuda:0\")), output_device=1)\n",
        "\n",
        "\n",
        "globalEncoder = nn.DataParallel(globalEncoder.cuda(), output_device=1)\n",
        "\n",
        "localRegressor = nn.DataParallel(localRegressor.cuda(), output_device=1) \n",
        "\n",
        "globalRegressor = nn.DataParallel(globalRegressor.cuda(), output_device=1)\n",
        "\n",
        "optimizer = torch.optim.Adam([{'params': globalEncoder.parameters()},\n",
        "                              {'params': localRegressor.parameters()},\n",
        "                              {'params': globalRegressor.parameters()}],\n",
        "                             lr = 1e-4,\n",
        "                             weight_decay=1e-5)\n",
        "\n",
        "dataset = AnomalyDataset(root_dir=f\"/content/drive/MyDrive/LOCO_AD/{target_dataset}\",\n",
        "                         transform = transforms.Compose([\n",
        "                            transforms.Resize((256, 256)),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.5, 0.5, 0.5), (0.225, 0.225, 0.225))]),\n",
        "                        type='train',\n",
        "                        label=0                        \n",
        "                        )\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "print(\"Preprocessing of training dataset\")\n",
        "with torch.no_grad():\n",
        "    t_mu, t_var, N = 0, 0, 0\n",
        "    for i, batch in tqdm(enumerate(dataloader)):\n",
        "        inputs = batch['image'].cuda()\n",
        "        t_out = localEncoder(inputs,fdfe=True)\n",
        "        t_mu, t_var, N = increment_mean_and_var(t_mu, t_var, N, t_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSEpfQSr4KSk"
      },
      "outputs": [],
      "source": [
        "def gcad_loss(loc_tch_output, loc_tch_output_patch, \n",
        "              glo_tch_output_up, glo_tch_output, \n",
        "              loc_stu_output, glo_stu_output):\n",
        "    \n",
        "    loc_tch_output = rearrange(loc_tch_output, 'b h w vec -> b vec h w')\n",
        "    loc_tch_output = conv11(loc_tch_output)\n",
        "    \n",
        "    l_kd = torch.mean(torch.norm(loc_tch_output - glo_tch_output_up, dim=1)**2)\n",
        "    l_loc = torch.mean(torch.norm(loc_tch_output_patch - loc_stu_output, dim=1)**2)\n",
        "    l_glo = torch.mean(torch.norm(glo_tch_output - glo_stu_output, dim=1)**2)\n",
        "    \n",
        "    return (l_kd / d_loc) + (l_loc / d_loc) + (l_glo / d_glo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9qKunfdb4KSk"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "random_crop = transforms.RandomCrop((33, 33))\n",
        "min_running_loss = np.inf\n",
        "stop_count = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    length = len(dataloader)\n",
        "    for i, batch in tqdm(enumerate(dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        inputs0 = batch['image'].cuda()\n",
        "        inputs1 = batch['image'].cuda()\n",
        "        cropped_inputs = random_crop(batch['image']).cuda()\n",
        "        with torch.no_grad():\n",
        "            targets = localEncoder(inputs0, fdfe=True) - t_mu / torch.sqrt(t_var)\n",
        "        loc_tch_patch = localEncoder(cropped_inputs)    \n",
        "        loc_stu = localRegressor(cropped_inputs)\n",
        "        \n",
        "        glo_tch = globalEncoder(inputs1)\n",
        "        glo_tch_up = globalEncoder(inputs1, upsample=True)\n",
        "        glo_stu = globalRegressor(inputs1)\n",
        "        \n",
        "        loss = gcad_loss(targets, loc_tch_patch, glo_tch_up, glo_tch, loc_stu, glo_stu)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    if running_loss < min_running_loss and epoch > 0:\n",
        "        torch.save(globalEncoder.module.state_dict(), \n",
        "                   f\"/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/global_encoder.pt\")\n",
        "        torch.save(localRegressor.module.state_dict(), \n",
        "                   f\"/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/local_regression.pt\")\n",
        "        torch.save(globalRegressor.module.state_dict(), \n",
        "                   f\"/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/global_regression.pt\")\n",
        "        print(f'Epoch #{epoch}  Loss decreased : {round(running_loss, 6)},  Model saved')\n",
        "        min_running_loss = running_loss\n",
        "        stop_count = 0\n",
        "    else:\n",
        "        stop_count += 1\n",
        "    if stop_count > early_stopping:\n",
        "        break\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbiE7A8L4KSl"
      },
      "outputs": [],
      "source": [
        "del dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDVlSfjG4KSl"
      },
      "outputs": [],
      "source": [
        "# Anomaly scoring\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from cv2 import GaussianBlur\n",
        "import cv2\n",
        "\n",
        "\n",
        "def get_error_map(students_pred, teacher_pred):\n",
        "    # student: (batch, student_id, h, w, vector)\n",
        "    # teacher: (batch, h, w, vector)\n",
        "    mu_students = reduce(students_pred, 'b id h w vec -> b h w vec', 'mean') # no change\n",
        "    err = reduce((mu_students - teacher_pred)**2, 'b h w vec -> b h w', 'sum') # teacher~student dist (norm)\n",
        "    return err\n",
        "\n",
        "def get_error_map_glo(students_pred, teacher_pred):\n",
        "    # student: (batch, student_id, h, w, vector)\n",
        "    # teacher: (batch, h, w, vector)\n",
        "    mu_students = reduce(students_pred, 'b id h w vec -> b h w vec', 'mean') # no change\n",
        "    err = reduce((mu_students - teacher_pred)**2, 'b c h w -> b h w', 'sum') # teacher~student dist (norm)\n",
        "    return err\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def  (teacher, students, dataloader, device):\n",
        "    print('calibrating teacher on Student dataset.')\n",
        "    t_mu, t_var, t_N = 0, 0, 0\n",
        "    for _, batch in tqdm(enumerate(dataloader)):\n",
        "        inputs = batch['image'].to(device)\n",
        "        t_out = teacher(inputs, fdfe=True)\n",
        "        t_mu, t_var, t_N = increment_mean_and_var(t_mu, t_var, t_N, t_out)\n",
        "    \n",
        "    print('calibrating scoring parameters on Student dataset.')\n",
        "    max_err, max_var = 0, 0\n",
        "    mu_err, var_err, N_err = 0, 0, 0\n",
        "\n",
        "    for _, batch in tqdm(enumerate(dataloader)):\n",
        "        inputs = batch['image'].to(device)\n",
        "\n",
        "        t_out = (teacher(inputs, fdfe=True) - t_mu) / torch.sqrt(t_var)\n",
        "        s_out = torch.stack([student(inputs, fdfe=True) for student in students], dim=1)\n",
        "\n",
        "        s_err = get_error_map(s_out, t_out)\n",
        "        mu_err, var_err, N_err = increment_mean_and_var(mu_err, var_err, N_err, s_err)\n",
        "\n",
        "        max_err = max(max_err, torch.max(s_err))\n",
        "\n",
        "    return {\"teacher\" : {\"mu\": t_mu, \"var\": t_var},\n",
        "            \"students\": {\"mu\": mu_err, \"var\": var_err, \"max\": max_err}\n",
        "           }\n",
        "\n",
        "@torch.no_grad()\n",
        "def calibrate_glo(teacher, students, dataloader, device):\n",
        "    print('calibrating teacher on Student dataset.')\n",
        "    t_mu, t_var, t_N = 0, 0, 0\n",
        "    for _, batch in tqdm(enumerate(dataloader)):\n",
        "        inputs = batch['image'].to(device)\n",
        "        t_out = teacher(inputs)\n",
        "        t_mu, t_var, t_N = increment_mean_and_var(t_mu, t_var, t_N, t_out)\n",
        "    \n",
        "    print('calibrating scoring parameters on Student dataset.')\n",
        "    max_err, max_var = 0, 0\n",
        "    mu_err, var_err, N_err = 0, 0, 0\n",
        "\n",
        "    for _, batch in tqdm(enumerate(dataloader)):\n",
        "        inputs = batch['image'].to(device)\n",
        "\n",
        "        t_out = (teacher(inputs) - t_mu) / torch.sqrt(t_var)\n",
        "        s_out = torch.stack([student(inputs) for student in students], dim=1)\n",
        "\n",
        "        s_err = get_error_map(s_out, t_out)\n",
        "        mu_err, var_err, N_err = increment_mean_and_var(mu_err, var_err, N_err, s_err)\n",
        "\n",
        "        max_err = max(max_err, torch.max(s_err))\n",
        "\n",
        "    return {\"teacher\" : {\"mu\": t_mu, \"var\": t_var},\n",
        "            \"students\": {\"mu\": mu_err, \"var\": var_err, \"max\": max_err}\n",
        "           }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_score_map(inputs, teacher, students, params):\n",
        "    t_out = (teacher.fdfe(inputs) - params['teacher']['mu']) / torch.sqrt(params['teacher']['var'])\n",
        "    s_out = torch.stack([student.fdfe(inputs) for student in students], dim=1)\n",
        "\n",
        "    s_err = get_error_map(s_out, t_out)\n",
        "    score_map = (s_err - params['students']['mu']) / torch.sqrt(params['students']['var'])\\\n",
        "    \n",
        "    return score_map\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_score_map_glo(inputs, teacher, students, params):\n",
        "    t_out = (teacher(inputs) - params['teacher']['mu']) / torch.sqrt(params['teacher']['var'])\n",
        "    s_out = torch.stack([student(inputs) for student in students], dim=1)\n",
        "\n",
        "    s_err = get_error_map_glo(s_out, t_out)\n",
        "    score_map = (s_err - params['students']['mu']) / torch.sqrt(params['students']['var'])\\\n",
        "    \n",
        "    return score_map\n",
        "\n",
        "\n",
        "def visualize(img, gt, score_map, max_score, i, mode):\n",
        "    plt.figure(figsize=(13, 3))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f'Original image')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(torch.round(gt), cmap='gray')\n",
        "    plt.title(f'Ground thuth anomaly')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "#     plt.imshow(score_map, cmap='jet')\n",
        "     \n",
        "    score_map = (score_map - score_map.min()) / (score_map.max() - score_map.min())\n",
        "    score_map = score_map.numpy()\n",
        "    score_map = GaussianBlur(score_map, (7, 7), cv2.BORDER_DEFAULT)\n",
        "#     plt.imshow(img, cmap='gray', interpolation='none')\n",
        "    plt.imshow(score_map, cmap='jet', alpha=0.5, interpolation='none')\n",
        "    plt.colorbar(extend='both')\n",
        "    plt.title('Anomaly map')\n",
        "\n",
        "#     plt.clim(0, max_score)\n",
        "    plt.savefig(f'/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/results/{mode}/{target_dataset}_{i}.png')\n",
        "    plt.show(block=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vmnbuBFt4KSl"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Teacher network\n",
        "localEncoder = LocalBranchEncoder()\n",
        "localEncoder.load_state_dict(torch.load(f\"/content/drive/MyDrive/LOCO_AD/local_encoder_fixed.pt\"))\n",
        "localEncoder.eval().cuda()\n",
        "\n",
        "# Students networks\n",
        "localRegressor = [LocalBranchEncoder()]\n",
        "localRegressor[0].load_state_dict(torch.load(f\"/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/local_regression.pt\"))\n",
        "localRegressor[0].eval().cuda()\n",
        "\n",
        "globalEncoder = GlobalBranchEncoder()\n",
        "globalEncoder.load_state_dict(torch.load(f\"/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/global_encoder.pt\"))\n",
        "globalEncoder.eval().cuda()\n",
        "\n",
        "globalRegressor = [UNet(in_channels=3, n_classes=d_glo, padding=True, up_mode='upsample')]\n",
        "globalRegressor[0].load_state_dict(torch.load(f\"C/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/global_regression.pt\"))\n",
        "globalRegressor[0].cuda()\n",
        "\n",
        "# calibration on anomaly-free dataset\n",
        "calib_dataset = AnomalyDataset(\n",
        "                root_dir=f'/content/drive/MyDrive/LOCO_AD/{target_dataset}',\n",
        "                transform=transforms.Compose([\n",
        "                    transforms.Resize((256, 256)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.225, 0.225, 0.225))]),\n",
        "                type='train',\n",
        "                label=0)\n",
        "\n",
        "calib_dataloader = DataLoader(calib_dataset, \n",
        "                               batch_size=8, \n",
        "                               shuffle=False)\n",
        "\n",
        "params_loc = calibrate(localEncoder, localRegressor, calib_dataloader, device)\n",
        "params_glo = calibrate_glo(globalEncoder, globalRegressor, calib_dataloader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RT8xjISS4KSm"
      },
      "outputs": [],
      "source": [
        "# Load testing data\n",
        "test_dataset = AnomalyDataset(\n",
        "                root_dir=f\"/content/drive/MyDrive/LOCO_AD/{target_dataset}\",\n",
        "                transform=transforms.Compose([\n",
        "                    transforms.Resize((256, 256)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.225, 0.225, 0.225))]),\n",
        "                gt_transform=transforms.Compose([\n",
        "                    transforms.Resize((256, 256)),\n",
        "                    transforms.ToTensor()]),\n",
        "                type='test')\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, \n",
        "                             batch_size=1, \n",
        "                             shuffle=False)\n",
        "\n",
        "\n",
        "# Build anomaly map\n",
        "y_score_loc = np.array([])\n",
        "y_score_glo = np.array([])\n",
        "y_true = np.array([])\n",
        "test_iter = iter(test_dataloader)\n",
        "\n",
        "for mode in ['both', 'local', 'global']:\n",
        "    test_size = len(test_dataset)\n",
        "    for i in range(test_size):\n",
        "        batch = next(test_iter)\n",
        "        inputs = batch['image'].to(device)\n",
        "        gt = batch['gt'].cpu()\n",
        "\n",
        "        score_map_loc = get_score_map(inputs, localEncoder, localRegressor, params_loc).cpu()\n",
        "        cur_y_score_loc = rearrange(score_map_loc, 'b h w -> (b h w)').numpy()\n",
        "        y_score_loc = np.concatenate((y_score_loc, cur_y_score_loc))\n",
        "\n",
        "        score_map_glo = get_score_map_glo(inputs, globalEncoder, globalRegressor, params_glo).cpu()\n",
        "        cur_y_score_glo = rearrange(score_map_glo, 'b h w -> (b h w)').numpy()\n",
        "        y_score_glo = np.concatenate((y_score_glo, cur_y_score_glo))\n",
        "\n",
        "        if mode == 'both':\n",
        "            score_map = score_map_loc + score_map_glo\n",
        "        elif mode == 'local':\n",
        "            score_map = score_map_glo\n",
        "        elif mode == 'global':\n",
        "            score_map = score_map_loc\n",
        "        y_true = np.concatenate((y_true, rearrange(gt, 'b c h w -> (b c h w)').numpy()))\n",
        "\n",
        "        unorm = transforms.Normalize((-2.22, -2.22, -2.22), (4.44, 4.44, 4.44)) # get back to original image\n",
        "        max_score = \\\n",
        "          (params_loc['students']['max'] - params_loc['students']['mu']) / torch.sqrt(params_loc['students']['var'])\n",
        "        + (params_glo['students']['max'] - params_glo['students']['mu']) / torch.sqrt(params_glo['students']['var'])\n",
        "        img_in = rearrange(unorm(inputs).cpu(), 'b c h w -> b h w c')\n",
        "        gt_in = rearrange(gt, 'b c h w -> b h w c')\n",
        "\n",
        "    #         for b in range(32): # batchsize\n",
        "        b = 0\n",
        "        visualize(img_in[b, :, :, :].squeeze(), \n",
        "                  gt_in[b, :, :, :].squeeze(), \n",
        "                  score_map[b, :, :].squeeze(), \n",
        "                  max_score, i, mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if5iSK_x4KSm"
      },
      "outputs": [],
      "source": [
        "print(\"*********** loop done ***********\")\n",
        "y_score = y_score_loc + y_score_glo\n",
        "y_score = (y_score - y_score.min()) / (y_score.max() - y_score.min()) # min max norm (0~1)\n",
        "\n",
        "# AUC ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_true.astype(int), (y_score))\n",
        "plt.figure(figsize=(13, 3))\n",
        "plt.plot(fpr, tpr, 'r', label=\"ROC\")\n",
        "plt.plot(fpr, fpr, 'b', label=\"random\")\n",
        "plt.title(f'ROC AUC: {auc(fpr, tpr)}')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.savefig(f\"/content/drive/MyDrive/LOCO_AD/gcad/{target_dataset}_{time_stamp}/results/{target_dataset}_roc.png\")\n",
        "#     plt.clf()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVnTcR5H4KSm"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_true.astype(int), 1-y_score)\n",
        "plt.figure(figsize=(13, 3))\n",
        "plt.plot(fpr, tpr, 'r', label=\"ROC\")\n",
        "plt.plot(fpr, fpr, 'b', label=\"random\")\n",
        "plt.title(f'ROC AUC: {auc(fpr, tpr)}')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# local_branch_anomaly_score()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r4NNyAO4KSm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}